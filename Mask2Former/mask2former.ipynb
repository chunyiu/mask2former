{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b32449",
   "metadata": {
    "papermill": {
     "duration": 8.305282,
     "end_time": "2024-11-08T04:23:53.470399",
     "exception": false,
     "start_time": "2024-11-08T04:23:45.165117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchmetrics.functional import dice\n",
    "from transformers import Mask2FormerForUniversalSegmentation\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from functools import partial\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device and seed setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1bbadc",
   "metadata": {
    "papermill": {
     "duration": 0.003998,
     "end_time": "2024-11-08T04:23:53.480960",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.476962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4610b",
   "metadata": {
    "papermill": {
     "duration": 0.025231,
     "end_time": "2024-11-08T04:23:53.510229",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.484998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IDMapper:\n",
    "    def __init__(self, name=''):\n",
    "        self.original_to_consecutive = {}\n",
    "        self.consecutive_to_original = {}\n",
    "        self.next_id = 0\n",
    "        self.name = name\n",
    "\n",
    "    def fit(self, id_lists):\n",
    "        # Fit the mapper to a collection of IDs.\n",
    "        unique_ids = set()\n",
    "        if id_lists and isinstance(id_lists[0], (list, set)):\n",
    "            # Handle nested lists (for attributes)\n",
    "            for id_list in id_lists:\n",
    "                unique_ids.update(id_list)\n",
    "        else:\n",
    "            # Handle flat lists (for categories)\n",
    "            unique_ids.update(id_lists)\n",
    "        \n",
    "        for original_id in sorted(unique_ids):\n",
    "            self.original_to_consecutive[original_id] = self.next_id\n",
    "            self.consecutive_to_original[self.next_id] = original_id\n",
    "            self.next_id += 1\n",
    "    \n",
    "    def transform(self, ids):\n",
    "        # Transform original IDs to consecutive IDs.\n",
    "        if isinstance(ids, (list, set)):\n",
    "            return [self.original_to_consecutive[id_] for id_ in ids]\n",
    "        return self.original_to_consecutive[ids]\n",
    "    \n",
    "    def inverse_transform(self, consecutive_ids):\n",
    "        # Transform consecutive IDs back to original IDs.\n",
    "        if isinstance(consecutive_ids, (list, set)):\n",
    "            return [self.consecutive_to_original[id_] for id_ in consecutive_ids]\n",
    "        return self.consecutive_to_original[consecutive_ids]\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        mapping_data = {\n",
    "            'original_to_consecutive': {str(k): v for k, v in self.original_to_consecutive.items()},\n",
    "            'consecutive_to_original': {str(k): v for k, v in self.consecutive_to_original.items()},\n",
    "            'next_id': self.next_id,\n",
    "            'name': self.name\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(mapping_data, f, indent=2)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            mapping_data = json.load(f)\n",
    "        \n",
    "        self.original_to_consecutive = {int(k): v for k, v in mapping_data['original_to_consecutive'].items()}\n",
    "        self.consecutive_to_original = {int(k): v for k, v in mapping_data['consecutive_to_original'].items()}\n",
    "        self.next_id = mapping_data['next_id']\n",
    "        self.name = mapping_data.get('name', '')\n",
    "    \n",
    "    @property\n",
    "    def num_ids(self):\n",
    "        # Return total number of mapped IDs\n",
    "        return self.next_id\n",
    "\n",
    "class FashionpediaDataset(Dataset):\n",
    "    def __init__(self, csv_file: str, img_dir: str, category_mapping_file: str = None, attribute_mapping_file: str = None):\n",
    "        self.csv_data = self.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "        # Initialize mappers\n",
    "        self.category_mapper = IDMapper(name='category')\n",
    "        self.attribute_mapper = IDMapper(name='attribute')\n",
    "        \n",
    "        # Handle category mapping\n",
    "        if category_mapping_file and os.path.exists(category_mapping_file):\n",
    "            self.category_mapper.load(category_mapping_file)\n",
    "        else:\n",
    "            all_category_ids = []\n",
    "            for item in self.csv_data:\n",
    "                all_category_ids.extend(item['CategoryId'])\n",
    "            self.category_mapper.fit(all_category_ids)\n",
    "            if category_mapping_file:\n",
    "                self.category_mapper.save(category_mapping_file)\n",
    "        \n",
    "        # Handle attribute mapping\n",
    "        if attribute_mapping_file and os.path.exists(attribute_mapping_file):\n",
    "            self.attribute_mapper.load(attribute_mapping_file)\n",
    "        else:\n",
    "            all_attribute_lists = []\n",
    "            for item in self.csv_data:\n",
    "                all_attribute_lists.extend(item['AttributesIds'])\n",
    "            self.attribute_mapper.fit(all_attribute_lists)\n",
    "            if attribute_mapping_file:\n",
    "                self.attribute_mapper.save(attribute_mapping_file)\n",
    "        \n",
    "        # Transform IDs to consecutive ones\n",
    "        for item in self.csv_data:\n",
    "            item['CategoryId'] = self.category_mapper.transform(item['CategoryId'])\n",
    "            item['AttributesIds'] = [self.attribute_mapper.transform(attr_list) for attr_list in item['AttributesIds']]\n",
    "\n",
    "    def read_csv(self, csv_file: str) -> List[Dict[str, Any]]:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # Group by ImageId to handle multiple segmentations per image\n",
    "        grouped = df.groupby('ImageId')\n",
    "        data = []\n",
    "        for _, group in grouped:\n",
    "            item = {\n",
    "                'ImageId': group.iloc[0]['ImageId'],\n",
    "                'Height': group.iloc[0]['Height'],\n",
    "                'Width': group.iloc[0]['Width'],\n",
    "                'EncodedPixels': [],\n",
    "                'CategoryId': [],\n",
    "                'AttributesIds': []\n",
    "            }\n",
    "            \n",
    "            # Collect data for each segmentation\n",
    "            for _, row in group.iterrows():\n",
    "                item['EncodedPixels'].append(row['EncodedPixels'])\n",
    "                item['CategoryId'].append(row['CategoryId'])\n",
    "                item['AttributesIds'].append([int(id) for id in row['AttributesIds'].split(',')])\n",
    "\n",
    "            data.append(item)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = {}\n",
    "\n",
    "        # Prepare pixel values\n",
    "        img_data = self.csv_data[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_data['ImageId']}.jpg\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        transform = T.Compose([\n",
    "        T.Resize((384, 384)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                   std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        image = transform(image)\n",
    "        \n",
    "        inputs['pixel_values'] = torch.tensor(image).unsqueeze(0)\n",
    "\n",
    "        # Create a multi-class segmentation mask\n",
    "        masks = []\n",
    "        for i, encoded_pixels in enumerate(img_data['EncodedPixels'], start=1):\n",
    "            mask = np.zeros((int(img_data['Height']), int(img_data['Width'])), dtype=np.int32)\n",
    "            if isinstance(encoded_pixels, str):\n",
    "                encoded_pixels = [int(x) for x in encoded_pixels.split()]\n",
    "                for j in range(0, len(encoded_pixels), 2):\n",
    "                    start, length = encoded_pixels[j] - 1, encoded_pixels[j + 1]\n",
    "                    row = start // int(img_data['Width'])\n",
    "                    col = start % int(img_data['Width'])\n",
    "                    mask[row:row+length, col] = i\n",
    "            mask = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0).float()\n",
    "            mask = F.interpolate(mask, size=(384, 384), mode='bilinear', align_corners=False)\n",
    "            mask = mask.squeeze(0).squeeze(0)\n",
    "            masks.append(mask)\n",
    "        \n",
    "        inputs['pixel_masks'] = torch.stack(masks).unsqueeze(0)\n",
    "        \n",
    "        # Prepare category labels\n",
    "        inputs['category_labels'] = torch.tensor(img_data['CategoryId']).unsqueeze(0)\n",
    "\n",
    "        # Prepare one-hot encoded attribute labels\n",
    "        num_attributes = self.attribute_mapper.num_ids\n",
    "        num_segments = len(img_data['AttributesIds'])\n",
    "        \n",
    "        one_hot_attributes = torch.zeros(num_segments, num_attributes)\n",
    "        \n",
    "        for segment_idx, attr_ids in enumerate(img_data['AttributesIds']):\n",
    "            if attr_ids:  # Check if there are any attributes for this segment\n",
    "                one_hot_attributes[segment_idx, attr_ids] = 1\n",
    "        \n",
    "        inputs['attribute_labels'] = one_hot_attributes.unsqueeze(0)\n",
    "\n",
    "        return inputs # Inputs for one image\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch]).squeeze(0)\n",
    "    pixel_masks = torch.stack([item['pixel_masks'] for item in batch]).squeeze(0).squeeze(0)\n",
    "    category_labels = torch.stack([item['category_labels'] for item in batch]).squeeze(0).squeeze(0)\n",
    "    attribute_labels = torch.stack([item['attribute_labels'] for item in batch]).squeeze(0).squeeze(0)\n",
    "\n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'pixel_masks': pixel_masks,\n",
    "        'category_labels': category_labels,\n",
    "        'attribute_labels': attribute_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c961e59",
   "metadata": {
    "papermill": {
     "duration": 0.004046,
     "end_time": "2024-11-08T04:23:53.518629",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.514583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46827ee7",
   "metadata": {
    "papermill": {
     "duration": 0.015664,
     "end_time": "2024-11-08T04:23:53.538258",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.522594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UncertaintyWeights(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize log variances for each task\n",
    "        self.log_var_category = nn.Parameter(torch.zeros(1))\n",
    "        self.log_var_attribute = nn.Parameter(torch.zeros(1))\n",
    "        self.log_var_mask = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self):\n",
    "        # Return precision weights and regularization terms\n",
    "        precision_category = torch.exp(-self.log_var_category)\n",
    "        precision_attribute = torch.exp(-self.log_var_attribute)\n",
    "        precision_mask = torch.exp(-self.log_var_mask)\n",
    "        \n",
    "        return {\n",
    "            'category': precision_category,\n",
    "            'attribute': precision_attribute,\n",
    "            'mask': precision_mask,\n",
    "            'regularization': (self.log_var_category + self.log_var_attribute + self.log_var_mask)\n",
    "        }\n",
    "\n",
    "def compute_miou(mask_logits, pixel_masks):\n",
    "    # Convert logits to binary predictions\n",
    "    mask_preds = (torch.sigmoid(mask_logits) > 0.5).float()\n",
    "    \n",
    "    # Ensure pixel_masks are the same size as predictions\n",
    "    if pixel_masks.shape[-2:] != mask_preds.shape[-2:]:\n",
    "        pixel_masks = F.interpolate(\n",
    "            pixel_masks.unsqueeze(1) if pixel_masks.dim() == 3 else pixel_masks,\n",
    "            size=mask_preds.shape[-2:],\n",
    "            mode='nearest'  # Using nearest neighbor to preserve binary values\n",
    "        )\n",
    "        pixel_masks = pixel_masks.squeeze(1) if pixel_masks.dim() == 4 else pixel_masks\n",
    "\n",
    "    # Hungarian matching\n",
    "    mask_preds = mask_preds.reshape(mask_preds.shape[0], -1)\n",
    "    pixel_masks = pixel_masks.reshape(pixel_masks.shape[0], -1)\n",
    "\n",
    "    intersection = torch.matmul(mask_preds, pixel_masks.T)  # Matrix multiplication to get pairwise intersections\n",
    "\n",
    "    union = torch.sum(mask_preds.unsqueeze(1) + pixel_masks.unsqueeze(0), dim=-1) - intersection  # Pairwise unions\n",
    "\n",
    "    iou = intersection / (union + 1e-6)\n",
    "\n",
    "    topk_ious, topk_indices = torch.topk(torch.flatten(iou), k=pixel_masks.shape[0], dim=0)\n",
    "    \n",
    "    miou = torch.mean(topk_ious)\n",
    "    best_indices = topk_indices // pixel_masks.shape[0]\n",
    "\n",
    "    return miou, best_indices\n",
    "\n",
    "def compute_category_loss(category_logits, category_labels):\n",
    "    num_predictions, num_classes = category_logits.shape\n",
    "    num_labels = category_labels.shape[0]\n",
    "\n",
    "    # Expand logits and labels to calculate pairwise losses\n",
    "    expanded_logits = category_logits.unsqueeze(1).expand(-1, num_labels, -1)\n",
    "    expanded_labels = category_labels.unsqueeze(0).expand(num_predictions, -1)\n",
    "\n",
    "    # Calculate pairwise cross-entropy losses\n",
    "    pairwise_losses = F.cross_entropy(\n",
    "        expanded_logits.reshape(-1, num_classes),\n",
    "        expanded_labels.reshape(-1),\n",
    "        reduction='none'\n",
    "    ).reshape(num_predictions, num_labels)\n",
    "\n",
    "    # Take the minimum loss for each prediction across the target labels\n",
    "    best_losses, _ = pairwise_losses.min(dim=0)\n",
    "    \n",
    "    # Compute the final mean of the best losses\n",
    "    category_loss = best_losses.mean()\n",
    "\n",
    "    return category_loss\n",
    "\n",
    "def compute_attribute_loss(attribute_logits, attribute_labels):\n",
    "    num_predictions, num_attributes = attribute_logits.shape\n",
    "    num_labels = attribute_labels.shape[0]\n",
    "\n",
    "    # Calculate pairwise binary cross-entropy losses\n",
    "    pairwise_losses = F.binary_cross_entropy_with_logits(\n",
    "        attribute_logits.unsqueeze(1).expand(-1, num_labels, -1),\n",
    "        attribute_labels.unsqueeze(0).expand(num_predictions, -1, -1),\n",
    "        reduction='none'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Calculate the total loss by summing across the attributes for each prediction-label pair\n",
    "    total_losses = pairwise_losses.mean(dim=-1)\n",
    "\n",
    "    # Select the lowest loss for each prediction across the 4 labels\n",
    "    best_losses, _ = total_losses.min(dim=1)\n",
    "    \n",
    "    # Compute the mean of the best losses\n",
    "    attribute_loss = best_losses.mean()\n",
    "\n",
    "    return attribute_loss\n",
    "\n",
    "def compute_weighted_loss(predictions, targets, uncertainty_weights):\n",
    "    category_logits, attribute_logits, mask_logits = predictions\n",
    "    category_labels, attribute_labels, pixel_masks = targets\n",
    "\n",
    "    # Mask Loss\n",
    "    mask_logits = mask_logits.squeeze(0)\n",
    "    miou, _ = compute_miou(mask_logits, pixel_masks)\n",
    "    mask_loss = 1 - miou\n",
    "\n",
    "    # Category Loss\n",
    "    category_logits = category_logits.squeeze(0)\n",
    "    category_loss = compute_category_loss(category_logits, category_labels)\n",
    "\n",
    "    # Attribute Loss\n",
    "    attribute_logits = attribute_logits.squeeze(0)\n",
    "    attribute_loss = compute_attribute_loss(attribute_logits, attribute_labels)\n",
    "\n",
    "    # Apply uncertainty weighting\n",
    "    weights = uncertainty_weights()\n",
    "    weighted_category_loss = weights['category'] * category_loss\n",
    "    weighted_attribute_loss = weights['attribute'] * attribute_loss\n",
    "    weighted_mask_loss = weights['mask'] * mask_loss\n",
    "    \n",
    "    # Combine losses with regularization term\n",
    "    total_loss = (\n",
    "        weighted_category_loss + \n",
    "        weighted_attribute_loss + \n",
    "        weighted_mask_loss + \n",
    "        0.5 * weights['regularization']\n",
    "    )\n",
    "    \n",
    "    return total_loss, category_loss, attribute_loss, mask_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95609f5c",
   "metadata": {
    "papermill": {
     "duration": 0.003915,
     "end_time": "2024-11-08T04:23:53.546375",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.542460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Custom Mask2Former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f0014",
   "metadata": {
    "papermill": {
     "duration": 0.009908,
     "end_time": "2024-11-08T04:23:53.560632",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.550724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomMask2FormerForFashionpedia(nn.Module):\n",
    "    def __init__(self, num_categories: int, num_attributes: int):\n",
    "        super().__init__()\n",
    "        self.mask2former = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-large-coco-instance\")\n",
    "\n",
    "        for param in self.mask2former.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.category_classifier = nn.Linear(self.mask2former.config.hidden_size, num_categories)\n",
    "        self.attribute_classifier = nn.Linear(self.mask2former.config.hidden_size, num_attributes)\n",
    "        \n",
    "        self.uncertainty_weights = UncertaintyWeights()\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        pixel_values,\n",
    "        pixel_masks,\n",
    "        category_labels,\n",
    "        attribute_labels\n",
    "    ):\n",
    "        outputs = self.mask2former(pixel_values=pixel_values)\n",
    "        mask_logits = outputs.masks_queries_logits\n",
    "\n",
    "        last_hidden_state = outputs.transformer_decoder_last_hidden_state\n",
    "        category_logits = self.category_classifier(last_hidden_state)\n",
    "        attribute_logits = self.attribute_classifier(last_hidden_state)\n",
    "\n",
    "        predictions = (category_logits, attribute_logits, mask_logits)\n",
    "        targets = (category_labels, attribute_labels, pixel_masks)\n",
    "\n",
    "        # Calculate uncertainty weighted loss\n",
    "        total_loss, category_loss, attribute_loss, mask_loss = compute_weighted_loss(\n",
    "            predictions, \n",
    "            targets,\n",
    "            self.uncertainty_weights\n",
    "        )\n",
    "\n",
    "        return total_loss, category_loss, attribute_loss, mask_loss, category_logits, attribute_logits, mask_logits\n",
    "    \n",
    "    def predict(self, pixel_values):\n",
    "        outputs = self.mask2former(pixel_values=pixel_values)\n",
    "        mask_logits = outputs.masks_queries_logits\n",
    "\n",
    "        last_hidden_state = outputs.transformer_decoder_last_hidden_state\n",
    "        category_logits = self.category_classifier(last_hidden_state)\n",
    "        attribute_logits = self.attribute_classifier(last_hidden_state)\n",
    "\n",
    "        return category_logits, attribute_logits, mask_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71a432",
   "metadata": {
    "papermill": {
     "duration": 0.004071,
     "end_time": "2024-11-08T04:23:53.568752",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.564681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e70df",
   "metadata": {
    "papermill": {
     "duration": 0.015952,
     "end_time": "2024-11-08T04:23:53.588745",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.572793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_weights(model, checkpoint_id):\n",
    "    checkpoint_path = f'mask2former_checkpoints/mask2former_checkpoint_{checkpoint_id}.pth'\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        print(f\" - Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        print(f\" - No checkpoint found at {checkpoint_path}\")\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs, checkpoint_id, device, gradient_accumulation_steps=4):\n",
    "\n",
    "    model = load_weights(model, checkpoint_id - 1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        optimizer.zero_grad()  # Zero gradients at start of epoch\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Transfer batch to device\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            pixel_masks = batch['pixel_masks'].to(device)\n",
    "            category_labels = batch['category_labels'].to(device)\n",
    "            attribute_labels = batch['attribute_labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            total_loss, _, _, _, _, _, _ = model(\n",
    "                pixel_values,\n",
    "                pixel_masks,\n",
    "                category_labels,\n",
    "                attribute_labels\n",
    "            )\n",
    "            \n",
    "            # Scale loss by gradient accumulation steps\n",
    "            scaled_loss = total_loss / gradient_accumulation_steps\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "            train_loss += total_loss.item()\n",
    "\n",
    "            # Step optimizer after accumulating gradients\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Explicit garbage collection\n",
    "                del total_loss, scaled_loss\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Clear memory for the batch\n",
    "            del pixel_values, pixel_masks, category_labels, attribute_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Handle any remaining gradients at end of epoch\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps != 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                pixel_masks = batch['pixel_masks'].to(device)\n",
    "                category_labels = batch['category_labels'].to(device)\n",
    "                attribute_labels = batch['attribute_labels'].to(device)\n",
    "\n",
    "                total_loss, _, _, _, _, _, _ = model(\n",
    "                    pixel_values,\n",
    "                    pixel_masks,\n",
    "                    category_labels,\n",
    "                    attribute_labels\n",
    "                )\n",
    "                \n",
    "                val_loss += total_loss.item()\n",
    "\n",
    "                # Clear memory after each validation batch\n",
    "                del pixel_values, pixel_masks, category_labels, attribute_labels\n",
    "                del total_loss\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"-----CHECKPOINT {checkpoint_id}-----\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Memory allocated: {torch.cuda.memory_allocated(device) / 1e9:.2f} GB\")\n",
    "        print(f\"Memory cached: {torch.cuda.memory_reserved(device) / 1e9:.2f} GB\")\n",
    "        print(f\"----------------------\")\n",
    "\n",
    "        # Save model with garbage collection\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss\n",
    "        }\n",
    "        torch.save(checkpoint, f'mask2former_checkpoints/mask2former_checkpoint_{checkpoint_id}.pth')\n",
    "        \n",
    "        # Force garbage collection at end of epoch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def plot_loss_curves(checkpoint_dir):\n",
    "    if not os.listdir(checkpoint_dir):\n",
    "        print(f\" - No checkpoint files found in '{checkpoint_dir}'.\")\n",
    "    else:\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        epochs = []\n",
    "        for filename in os.listdir(checkpoint_dir):\n",
    "            if filename.endswith('.pth'):\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "                checkpoint = torch.load(checkpoint_path)\n",
    "                train_losses.append(checkpoint['train_loss'])\n",
    "                val_losses.append(checkpoint['val_loss'])\n",
    "                epochs.append(checkpoint['epoch'])\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training vs Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig('mask2former_loss_plot.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea942e5",
   "metadata": {
    "papermill": {
     "duration": 0.004739,
     "end_time": "2024-11-08T04:23:53.597971",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.593232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c167a6f",
   "metadata": {
    "papermill": {
     "duration": 0.018899,
     "end_time": "2024-11-08T04:23:53.620888",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.601989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_best_model(model, checkpoint_dir):\n",
    "    val_losses = []\n",
    "    for filename in os.listdir(checkpoint_dir):\n",
    "        if filename.endswith(\".pth\"):\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            val_loss = checkpoint['val_loss']\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "    if val_losses:\n",
    "        best_index = val_losses.index(min(val_losses))\n",
    "        best_filename = os.listdir(checkpoint_dir)[best_index]\n",
    "        best_model_path = os.path.join(checkpoint_dir, best_filename)\n",
    "        best_checkpoint = torch.load(best_model_path)\n",
    "        print(f\" - Loading best model from {best_model_path}\")\n",
    "        model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        print(\" - No checkpoints found.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def compute_category_dice(category_predictions, category_labels):\n",
    "    num_predictions, _ = category_predictions.shape\n",
    "    num_labels = category_labels.shape[0]\n",
    "\n",
    "    # Calculate pairwise Dice scores\n",
    "    pairwise_dice_scores = torch.zeros((num_predictions, num_labels))\n",
    "\n",
    "    for i in range(num_predictions):\n",
    "        for j in range(num_labels):\n",
    "            # Calculate the Dice score for each pair of prediction and label\n",
    "            pairwise_dice_scores[i, j] = dice(\n",
    "                category_predictions[i].unsqueeze(0),\n",
    "                category_labels[j].unsqueeze(0).int(),\n",
    "                mdmc_average='global'\n",
    "            )\n",
    "\n",
    "    # Select the best Dice score for each prediction\n",
    "    best_dice_scores, _ = pairwise_dice_scores.max(dim=1)\n",
    "    best_dice_score = best_dice_scores.mean()\n",
    "\n",
    "    return best_dice_score\n",
    "\n",
    "def compute_attribute_dice(attribute_predictions, attribute_labels):\n",
    "    num_predictions, _ = attribute_predictions.shape\n",
    "    num_labels = attribute_labels.shape[0]\n",
    "\n",
    "    # Reshape and calculate pairwise Dice scores for each attribute\n",
    "    pairwise_dice = torch.zeros((num_predictions, num_labels), device=attribute_predictions.device)\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        # Calculate Dice score between each prediction and label\n",
    "        pairwise_dice[:, i] = dice(\n",
    "            attribute_predictions,\n",
    "            attribute_labels[i].unsqueeze(0).expand(num_predictions, -1).int(),\n",
    "            mdmc_average='samplewise'\n",
    "        )\n",
    "\n",
    "    # Select the best Dice score for each prediction\n",
    "    best_dice_scores, _ = pairwise_dice.max(dim=1)\n",
    "    attribute_dice = best_dice_scores.mean()\n",
    "\n",
    "    return attribute_dice\n",
    "\n",
    "def evaluate_model(model, test_loader, device, batch_size=32):\n",
    "    model.eval()\n",
    "    batch_results = []\n",
    "    current_batch = {\n",
    "        'mask_predictions': [], \n",
    "        'pixel_masks': [], \n",
    "        'category_predictions': [], \n",
    "        'category_labels': [], \n",
    "        'attribute_predictions': [], \n",
    "        'attribute_labels': [], \n",
    "    }\n",
    "    \n",
    "    def process_batch_metrics(batch_data):\n",
    "        # Calculate metrics for current batch\n",
    "        mask_predictions = torch.cat(batch_data['mask_predictions'], dim=0)\n",
    "        pixel_masks = torch.cat(batch_data['pixel_masks'], dim=0)\n",
    "        mask_miou, _ = compute_miou(mask_predictions, pixel_masks)\n",
    "\n",
    "        category_predictions = torch.cat(batch_data['category_predictions'],dim=0)\n",
    "        category_labels = torch.cat(batch_data['category_labels'],dim=0).int()\n",
    "        category_dice = compute_category_dice(category_predictions, category_labels)\n",
    "\n",
    "        attribute_predictions = torch.cat(batch_data['attribute_predictions'],dim=0)\n",
    "        attribute_labels = torch.cat(batch_data['attribute_labels'],dim=0).int()\n",
    "        attribute_dice = compute_attribute_dice(attribute_predictions, attribute_labels)\n",
    "\n",
    "        return {\n",
    "            'miou': mask_miou,\n",
    "            'category_dice': category_dice,\n",
    "            'attribute_dice': attribute_dice,\n",
    "            'batch_size': len(batch_data['mask_predictions'])\n",
    "        }\n",
    "    \n",
    "    def clear_batch_data(batch_data):\n",
    "        for key in batch_data:\n",
    "            batch_data[key] = []\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    progress_bar = tqdm(total=len(test_loader), desc=\"Evaluating\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            # Process batch\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            pixel_masks = batch['pixel_masks'].to(device)\n",
    "            category_labels = batch['category_labels'].to(device)\n",
    "            attribute_labels = batch['attribute_labels'].to(device)\n",
    "            \n",
    "            category_logits, attribute_logits, mask_logits = model.predict(pixel_values)\n",
    "            \n",
    "            mask_logits = mask_logits.squeeze(0)\n",
    "            current_batch['mask_predictions'].append(mask_logits)\n",
    "            current_batch['pixel_masks'].append(pixel_masks)\n",
    "            \n",
    "            category_logits = category_logits.squeeze(0)\n",
    "            current_batch['category_predictions'].append(category_logits)\n",
    "            current_batch['category_labels'].append(category_labels)\n",
    "            \n",
    "            attribute_logits = attribute_logits.squeeze(0)\n",
    "            current_batch['attribute_predictions'].append(attribute_logits)\n",
    "            current_batch['attribute_labels'].append(attribute_labels)\n",
    "            \n",
    "            # Clear immediate memory\n",
    "            del pixel_values, pixel_masks, category_labels, attribute_labels\n",
    "            del category_logits, attribute_logits, mask_logits\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            # Process batch if reached batch_size\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                batch_metrics = process_batch_metrics(current_batch)\n",
    "                batch_results.append(batch_metrics)\n",
    "                clear_batch_data(current_batch)\n",
    "\n",
    "            # Number of samples to evaluate\n",
    "            if i == 1024:\n",
    "                break\n",
    "\n",
    "    # Calculate final metrics by summing values and dividing by the number of batches\n",
    "    num_batches = len(batch_results)\n",
    "\n",
    "    final_miou = sum(result['miou'] for result in batch_results) / num_batches\n",
    "    final_category_dice = sum(result['category_dice'] for result in batch_results) / num_batches\n",
    "    final_attribute_dice = sum(result['attribute_dice'] for result in batch_results) / num_batches\n",
    "\n",
    "    # Save results\n",
    "    with open(\"mask2former_results.txt\", \"w\") as f:\n",
    "        f.write(\"-----RESULTS-----\\n\")\n",
    "        f.write(f\"Mean IoU: {final_miou:.4f}\\n\")\n",
    "        f.write(f\"Category Dice Coefficient: {final_category_dice:.4f}\\n\")\n",
    "        f.write(f\"Attributes Dice Coefficient: {final_attribute_dice:.4f}\\n\")\n",
    "        f.write(\"-----------------\")\n",
    "\n",
    "    with open(\"mask2former_results.txt\", \"r\") as f:\n",
    "        print(f.read())\n",
    "    \n",
    "    # Clear final memory\n",
    "    del batch_results, current_batch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return final_category_dice, final_attribute_dice, final_miou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e6ed95",
   "metadata": {
    "papermill": {
     "duration": 0.004018,
     "end_time": "2024-11-08T04:23:53.629114",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.625096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sample Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9358d60",
   "metadata": {
    "papermill": {
     "duration": 0.016468,
     "end_time": "2024-11-08T04:23:53.649803",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.633335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_category_attribute_names(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    categories = {\n",
    "        str(category['id']): category['name']\n",
    "        for category in data['categories']\n",
    "    }\n",
    "    \n",
    "    attributes = {\n",
    "        str(attr['id']): attr['name']\n",
    "        for attr in data['attributes']\n",
    "    }\n",
    "    return categories, attributes\n",
    "\n",
    "def compute_iou(mask1, mask2):\n",
    "    intersection = torch.logical_and(mask1, mask2).sum()\n",
    "    union = torch.logical_or(mask1, mask2).sum()\n",
    "    return (intersection / union).item() if union > 0 else 0\n",
    "\n",
    "def non_max_suppression(masks, confidence_scores, iou_threshold=0.5):\n",
    "    if len(masks) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Convert to numpy for easier indexing\n",
    "    scores = confidence_scores.cpu().numpy()\n",
    "    \n",
    "    # Get indices sorted by confidence\n",
    "    indices = scores.argsort()[::-1]\n",
    "    kept_indices = []\n",
    "    \n",
    "    while len(indices) > 0:\n",
    "        # Keep the mask with highest confidence\n",
    "        current_idx = indices[0]\n",
    "        kept_indices.append(current_idx)\n",
    "        \n",
    "        if len(indices) == 1:\n",
    "            break\n",
    "            \n",
    "        # Compute IoU between the current mask and all remaining masks\n",
    "        ious = []\n",
    "        current_mask = masks[current_idx]\n",
    "        \n",
    "        for idx in indices[1:]:\n",
    "            iou = compute_iou(current_mask, masks[idx])\n",
    "            ious.append(iou)\n",
    "        \n",
    "        # Filter out masks with IoU above threshold\n",
    "        indices = indices[1:][np.array(ious) < iou_threshold]\n",
    "    \n",
    "    return kept_indices\n",
    "\n",
    "def visualize_sample(model, dataset, idx, device, labels_file, save_path='mask2former_sample_segmentation.png'):\n",
    "    # Get sample and prepare for model\n",
    "    sample = dataset[idx]\n",
    "    pixel_values = sample['pixel_values'].to(device)\n",
    "    \n",
    "    # Get image for visualization\n",
    "    img_data = dataset.csv_data[idx]\n",
    "    img_path = os.path.join(dataset.img_dir, f\"{img_data['ImageId']}.jpg\")\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    img_width, img_height = image.size\n",
    "    \n",
    "    # Get model predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        category_logits, attribute_logits, mask_logits = model.predict(pixel_values)\n",
    "    \n",
    "    initial_top_k = 100  # Initial top-k since NMS will filter some masks\n",
    "\n",
    "    # Process masks\n",
    "    mask_probs = torch.sigmoid(mask_logits.squeeze(0))\n",
    "    mask_confidence = mask_probs.mean(dim=[-1, -2])\n",
    "    top_mask_indices = mask_confidence.argsort(descending=True)[:initial_top_k]\n",
    "    mask_logits = mask_logits.squeeze(0)[top_mask_indices]\n",
    "\n",
    "    mask_logits = F.interpolate(\n",
    "        mask_logits.unsqueeze(1) if mask_logits.dim() == 3 else mask_logits,\n",
    "        size=(img_height, img_width),\n",
    "        mode='bilinear'\n",
    "    )\n",
    "    mask_logits = mask_logits.squeeze(1) if mask_logits.dim() == 4 else mask_logits\n",
    "    predicted_masks = torch.sigmoid(mask_logits) > 0.5\n",
    "    \n",
    "    # Apply NMS to filter overlapping masks\n",
    "    kept_indices = non_max_suppression(\n",
    "        predicted_masks,\n",
    "        mask_confidence[top_mask_indices],\n",
    "        iou_threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # Update predictions based on NMS results\n",
    "    predicted_masks = predicted_masks[kept_indices]\n",
    "    top_mask_indices = top_mask_indices[kept_indices]\n",
    "    \n",
    "    # Limit to top_k after NMS if necessary\n",
    "    final_top_k = 15\n",
    "    if len(predicted_masks) > final_top_k:\n",
    "        predicted_masks = predicted_masks[:final_top_k]\n",
    "        top_mask_indices = top_mask_indices[:final_top_k]\n",
    "    \n",
    "    # Process categories\n",
    "    category_logits = category_logits.squeeze(0)[top_mask_indices]\n",
    "    category_probs = F.softmax(category_logits, dim=-1)\n",
    "    predicted_categories = torch.argmax(category_probs, dim=-1)\n",
    "    \n",
    "    # Process attributes\n",
    "    attribute_logits = attribute_logits.squeeze(0)[top_mask_indices]\n",
    "    attribute_probs = torch.sigmoid(attribute_logits)\n",
    "    ohe_predicted_attributes = (attribute_probs > 0.5).float()\n",
    "    \n",
    "    # Retrieve original category and attribute names\n",
    "    category_map, attribute_map = load_category_attribute_names(labels_file)\n",
    "\n",
    "    # Reverse attributes one hot encoding\n",
    "    predicted_attribute_ids = []\n",
    "    ohe_predicted_attributes = ohe_predicted_attributes.tolist()\n",
    "    for ohe_attr in ohe_predicted_attributes:\n",
    "        attr_list = []\n",
    "        for i, value in enumerate(ohe_attr):\n",
    "            if value == 1:\n",
    "                attr_list.append(i)\n",
    "        predicted_attribute_ids.append(attr_list)\n",
    "\n",
    "    predicted_category_ids = dataset.category_mapper.inverse_transform(predicted_categories.tolist())\n",
    "    predicted_category_names = [category_map[str(pred_cat_id)] for pred_cat_id in predicted_category_ids]\n",
    "\n",
    "    predicted_attribute_ids = [dataset.attribute_mapper.inverse_transform(attr_list) for attr_list in predicted_attribute_ids]\n",
    "    predicted_attribute_names = [[attribute_map[str(pred_attr_id)] for pred_attr_id in attr_list] for attr_list in predicted_attribute_ids]\n",
    "\n",
    "    # Create figure with custom layout\n",
    "    fig = plt.figure(figsize=(13, 7))\n",
    "    gs = plt.GridSpec(1, 2)\n",
    "    \n",
    "    # Main image plot\n",
    "    ax_main = fig.add_subplot(gs[0])\n",
    "    ax_main.imshow(image)\n",
    "    \n",
    "    # Generate distinct colors for each mask using HSV color space\n",
    "    num_masks = len(predicted_masks)\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, num_masks))\n",
    "    \n",
    "    # Plot masks on main image\n",
    "    for mask, color in zip(predicted_masks, colors):\n",
    "        mask_array = mask.cpu().numpy()\n",
    "        mask_overlay = np.zeros((img_height, img_width, 4))\n",
    "        mask_overlay[mask_array] = (*color[:3], 0.3)  # RGB + alpha\n",
    "        ax_main.imshow(mask_overlay)\n",
    "    \n",
    "    ax_main.axis('off')\n",
    "    ax_main.set_title(f'Image {idx}', pad=10)\n",
    "    \n",
    "    # Side panel for legend\n",
    "    ax_legend = fig.add_subplot(gs[1])\n",
    "    ax_legend.axis('off')\n",
    "    \n",
    "    # Create legend content\n",
    "    legend_text = []\n",
    "    for i, (category, attributes, color) in enumerate(zip(predicted_category_names, \n",
    "                                                        predicted_attribute_names, \n",
    "                                                        colors)):\n",
    "        # Limit attributes to 5\n",
    "        limited_attributes = attributes[:5]\n",
    "        \n",
    "        # Create a line-like object for the legend\n",
    "        line = Line2D([0], [0], color=color, lw=4, alpha=0.3)\n",
    "        ax_legend.add_line(line)\n",
    "        legend_text.append(f\"Category: {category} | Attributes: {', '.join(limited_attributes)}\")\n",
    "\n",
    "    # Create the legend\n",
    "    ax_legend.legend(legend_text, loc='center right', frameon=False)\n",
    "    \n",
    "    # Save and show the visualization\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f335d1",
   "metadata": {
    "papermill": {
     "duration": 0.003911,
     "end_time": "2024-11-08T04:23:53.658012",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.654101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db157e3d",
   "metadata": {
    "papermill": {
     "duration": 0.007588,
     "end_time": "2024-11-08T04:23:53.669799",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.662211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_file = 'fashionpedia_data/annotations.csv'\n",
    "img_dir = 'fashionpedia_data/images'\n",
    "category_mapping_file = 'fashionpedia_data/consecutive_category_mapping.json'\n",
    "attribute_mapping_file = 'fashionpedia_data/consecutive_attribute_mapping.json'\n",
    "labels_file = 'fashionpedia_data/labels.json'\n",
    "checkpoint_dir = 'mask2former_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55863e",
   "metadata": {
    "papermill": {
     "duration": 18.895867,
     "end_time": "2024-11-08T04:24:12.569908",
     "exception": false,
     "start_time": "2024-11-08T04:23:53.674041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = FashionpediaDataset(csv_file, img_dir, category_mapping_file, attribute_mapping_file)\n",
    "print(' - Dataset created.')\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "print(' - Dataset split into train, val and test.')\n",
    "\n",
    "# Create data loaders\n",
    "collate_fn = partial(collate_fn)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "print(' - Train, val and test loaders initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3639968",
   "metadata": {
    "papermill": {
     "duration": 3.153729,
     "end_time": "2024-11-08T04:24:15.739479",
     "exception": false,
     "start_time": "2024-11-08T04:24:12.585750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = CustomMask2FormerForFashionpedia(num_categories=dataset.category_mapper.num_ids, num_attributes=dataset.attribute_mapper.num_ids).to(device)\n",
    "print(' - CustomMask2FormerforFashionpedia model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f3c60",
   "metadata": {
    "papermill": {
     "duration": 17355.58711,
     "end_time": "2024-11-08T09:13:31.345177",
     "exception": false,
     "start_time": "2024-11-08T04:24:15.758067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN = 1 # Toggle training\n",
    "\n",
    "if TRAIN:\n",
    "    checkpoint_id = 29\n",
    "    num_epochs = 1\n",
    "    gradient_accumulation_steps = 32\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    print(' - Optimizer set up.')\n",
    "\n",
    "    print(' - Model training initiated.')\n",
    "    train_model(model, train_loader, val_loader, optimizer, num_epochs, checkpoint_id, device, gradient_accumulation_steps=gradient_accumulation_steps)\n",
    "    print(' - Model training completed.')\n",
    "    print(f' - Checkpoint {checkpoint_id} saved to mask2former_checkpoints.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9ab8c",
   "metadata": {
    "papermill": {
     "duration": 1.88907,
     "end_time": "2024-11-08T09:13:33.241018",
     "exception": false,
     "start_time": "2024-11-08T09:13:31.351948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training vs validation loss\n",
    "plot_loss_curves(checkpoint_dir)\n",
    "print(' - Loss curves plotted.')\n",
    "print(' - Loss plot saved to mask2former_loss_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d45d9",
   "metadata": {
    "papermill": {
     "duration": 1251.852294,
     "end_time": "2024-11-08T09:34:25.100871",
     "exception": false,
     "start_time": "2024-11-08T09:13:33.248577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVALUATE = 1 # Toggle evaluation\n",
    "\n",
    "if EVALUATE:\n",
    "    # Load the best model\n",
    "    model = load_best_model(model, checkpoint_dir)\n",
    "    print(' - Best model loaded.')\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(' - Model evaluation initiated.')\n",
    "    category_f1, attribute_f1, miou = evaluate_model(model, test_loader, device)\n",
    "    print(' - Model evaluation complete.')\n",
    "    print(' - Results saved to mask2former_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002aad75",
   "metadata": {
    "papermill": {
     "duration": 0.020856,
     "end_time": "2024-11-08T09:34:25.139046",
     "exception": false,
     "start_time": "2024-11-08T09:34:25.118190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAMPLE = 1 # Toggle visualising sample\n",
    "\n",
    "if SAMPLE:\n",
    "    # Load the best model\n",
    "    model = load_best_model(model, checkpoint_dir)\n",
    "\n",
    "    # Visualise a image segmentation sample\n",
    "    print(' - Generating sample segmentation.')\n",
    "    visualize_sample(model, dataset, idx=5999, device=device, labels_file=labels_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18643.489613,
   "end_time": "2024-11-08T09:34:27.617862",
   "environment_variables": {},
   "exception": null,
   "input_path": "mask2former1.ipynb",
   "output_path": "mask2former1_output.ipynb",
   "parameters": {},
   "start_time": "2024-11-08T04:23:44.128249",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
